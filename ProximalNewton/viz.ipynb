{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de922a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running L1-regularized logistic regression test...\n",
      "Iteration 0: objective = 6.654202e-01, grad_norm = 3.285919e-01\n",
      "Iteration 5: objective = 6.623422e-01, grad_norm = 3.238147e-01\n",
      "Iteration 10: objective = 6.623384e-01, grad_norm = 3.238087e-01\n",
      "Iteration 15: objective = 6.623381e-01, grad_norm = 3.238082e-01\n",
      "Iteration 20: objective = 6.623378e-01, grad_norm = 3.238077e-01\n",
      "Iteration 25: objective = 6.623375e-01, grad_norm = 3.238072e-01\n",
      "Maximum iterations (30) reached.\n",
      "Running Hessian approximation comparison test...\n",
      "Testing exact Hessian approximation...\n",
      "Iteration 0: objective = 1.758609e+01, grad_norm = 3.780817e+00\n",
      "Testing l-bfgs Hessian approximation...\n",
      "Iteration 0: objective = 1.543365e+01, grad_norm = 2.987556e+00\n",
      "Iteration 5: objective = 1.186029e+01, grad_norm = 1.890354e-03\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 10: objective = 1.186029e+01, grad_norm = 1.241505e-06\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Running subproblem solver comparison test...\n",
      "Testing prox-gradient subproblem solver...\n",
      "Iteration 0: objective = -6.437552e+00, grad_norm = 5.593962e-01\n",
      "Iteration 5: objective = -6.506103e+00, grad_norm = 5.455285e-01\n",
      "Iteration 10: objective = -6.506103e+00, grad_norm = 5.455285e-01\n",
      "Iteration 15: objective = -6.506103e+00, grad_norm = 5.455285e-01\n",
      "Iteration 20: objective = -6.506103e+00, grad_norm = 5.455285e-01\n",
      "Iteration 25: objective = -6.506103e+00, grad_norm = 5.455285e-01\n",
      "Maximum iterations (30) reached.\n",
      "Testing coordinate-descent subproblem solver...\n",
      "Iteration 0: objective = -5.234783e+00, grad_norm = 9.681644e-01\n",
      "Iteration 5: objective = -6.440646e+00, grad_norm = 5.279772e-01\n",
      "Iteration 10: objective = -6.454457e+00, grad_norm = 5.232605e-01\n",
      "Iteration 15: objective = -6.454679e+00, grad_norm = 5.232045e-01\n",
      "Iteration 20: objective = -6.454681e+00, grad_norm = 5.232036e-01\n",
      "Iteration 25: objective = -6.454681e+00, grad_norm = 5.232036e-01\n",
      "Maximum iterations (30) reached.\n",
      "Testing admm subproblem solver...\n",
      "Iteration 0: objective = -5.968767e+00, grad_norm = 1.702659e+00\n",
      "Iteration 5: objective = -6.288256e+00, grad_norm = 6.006109e-01\n",
      "Iteration 10: objective = -6.417129e+00, grad_norm = 5.696623e-01\n",
      "Iteration 15: objective = -6.506104e+00, grad_norm = 5.455283e-01\n",
      "Iteration 20: objective = -6.506104e+00, grad_norm = 5.455283e-01\n",
      "Iteration 25: objective = -6.506104e+00, grad_norm = 5.455283e-01\n",
      "Maximum iterations (30) reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_31032\\3331402139.py:273: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: objective = -1.007049e+03, grad_norm = 5.012589e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 5: objective = -1.007049e+03, grad_norm = 5.012599e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 10: objective = -1.007049e+03, grad_norm = 5.012609e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 15: objective = -1.007049e+03, grad_norm = 5.012618e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 20: objective = -1.007049e+03, grad_norm = 5.012628e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 25: objective = -1.007049e+03, grad_norm = 5.012638e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 30: objective = -1.007049e+03, grad_norm = 5.012647e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 35: objective = -1.007049e+03, grad_norm = 5.012657e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 40: objective = -1.007049e+03, grad_norm = 5.012666e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Iteration 45: objective = -1.007049e+03, grad_norm = 5.012676e-01\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Warning: not a descent direction. Using steepest descent.\n",
      "Maximum iterations (50) reached.\n",
      "Iteration 0: objective = -8.517998e+02, grad_norm = 3.011401e+02\n",
      "Iteration 5: objective = -9.913158e+02, grad_norm = 6.202764e+01\n",
      "Iteration 10: objective = -9.983708e+02, grad_norm = 1.775431e+01\n",
      "Iteration 15: objective = -1.001727e+03, grad_norm = 1.618848e+01\n",
      "Iteration 20: objective = -1.002998e+03, grad_norm = 1.360280e+01\n",
      "Iteration 25: objective = -1.004891e+03, grad_norm = 7.143976e+00\n",
      "Iteration 30: objective = -1.005357e+03, grad_norm = 4.867374e+00\n",
      "Iteration 35: objective = -1.005652e+03, grad_norm = 1.106731e+01\n",
      "Iteration 40: objective = -1.005847e+03, grad_norm = 4.161834e+00\n",
      "Iteration 45: objective = -1.005966e+03, grad_norm = 4.997744e+00\n",
      "Iteration 50: objective = -1.006249e+03, grad_norm = 3.083792e+00\n",
      "Iteration 55: objective = -1.006317e+03, grad_norm = 2.521217e+00\n",
      "Iteration 60: objective = -1.006441e+03, grad_norm = 2.879347e+00\n",
      "Iteration 65: objective = -1.006523e+03, grad_norm = 2.250827e+00\n",
      "Iteration 70: objective = -1.006584e+03, grad_norm = 2.143257e+00\n",
      "Iteration 75: objective = -1.006626e+03, grad_norm = 1.919919e+00\n",
      "Iteration 80: objective = -1.006674e+03, grad_norm = 3.595457e+00\n",
      "Iteration 85: objective = -1.006708e+03, grad_norm = 1.696840e+00\n",
      "Iteration 90: objective = -1.006744e+03, grad_norm = 3.117759e+00\n",
      "Iteration 95: objective = -1.006774e+03, grad_norm = 1.515316e+00\n",
      "Maximum iterations (100) reached.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ProximalNewton import ProximalNewton\n",
    "\n",
    "def run_convergence_tests():\n",
    "    \"\"\"\n",
    "    Run convergence tests for different Proximal Newton configurations and\n",
    "    visualize the results.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate the three specific visualizations\n",
    "    print(\"Running Hessian approximation comparison test...\")\n",
    "    visualize_hessian_approximations()\n",
    "    \n",
    "    print(\"Running subproblem solver comparison test...\")\n",
    "    visualize_subproblem_solvers()\n",
    "    \n",
    "\n",
    "def visualize_hessian_approximations():\n",
    "    \"\"\"\n",
    "    Compare different Hessian approximation methods.\n",
    "    \"\"\"\n",
    "    # Define problem\n",
    "    n = 20\n",
    "    \n",
    "    # Simple non-quadratic problem\n",
    "    def f(x):\n",
    "        return np.sum(np.log(1 + np.exp(x))) + 0.5 * np.sum(x**2)\n",
    "    \n",
    "    def grad(x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / (1 + exp_x) + x\n",
    "    \n",
    "    def hess(x):\n",
    "        exp_x = np.exp(x)\n",
    "        d = exp_x / (1 + exp_x)**2\n",
    "        return np.diag(d + 1.0)\n",
    "    \n",
    "    # Identity proximal operator (no regularization)\n",
    "    identity_prox = lambda x, t: x\n",
    "    \n",
    "    # Initial point\n",
    "    x0 = np.ones(n)\n",
    "    \n",
    "    # Test different Hessian approximations\n",
    "    hessian_types = ['exact', 'bfgs', 'l-bfgs']\n",
    "    results = {}\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    for i, hessian_type in enumerate(hessian_types):\n",
    "        print(f\"Testing {hessian_type} Hessian approximation...\")\n",
    "        \n",
    "        # Create Proximal Newton solver\n",
    "        pn = ProximalNewton(\n",
    "            f, grad, hess if hessian_type == 'exact' else None,\n",
    "            identity_prox,\n",
    "            max_iter=20, tol=1e-6,\n",
    "            hessian_type=hessian_type,\n",
    "            memory=10 if hessian_type == 'l-bfgs' else None,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Run optimization and get history\n",
    "        iterates, num_iters = pn.optimize(x0.copy(), return_iterates=True, return_iterations=True)\n",
    "        \n",
    "        # Compute objective values and gradient norms\n",
    "        obj_values = [f(x) for x in iterates]\n",
    "        grad_norms = [np.linalg.norm(grad(x)) for x in iterates]\n",
    "        \n",
    "        # Plot results\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.semilogy(obj_values, marker='o', label=hessian_type)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.semilogy(grad_norms, marker='o', label=hessian_type)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Objective value')\n",
    "    plt.title('Comparison of Hessian Approximations: Objective Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient norm')\n",
    "    plt.title('Comparison of Hessian Approximations: Gradient Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('proximal_newton_hessian_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_subproblem_solvers():\n",
    "    \"\"\"\n",
    "    Compare different subproblem solvers.\n",
    "    \"\"\"\n",
    "    # Create a regularized problem where the proximal operator is non-trivial\n",
    "    n = 30\n",
    "    \n",
    "    # Generate a synthetic problem\n",
    "    A = np.random.randn(n, n)\n",
    "    A = np.dot(A.T, A) + 0.1 * np.eye(n)  # Make positive definite\n",
    "    b = np.random.randn(n)\n",
    "    \n",
    "    # Add the L1 norm as a regularizer\n",
    "    alpha = 0.1\n",
    "    \n",
    "    def f_smooth(x):\n",
    "        return 0.5 * np.dot(x, np.dot(A, x)) - np.dot(b, x)\n",
    "    \n",
    "    def grad_smooth(x):\n",
    "        return np.dot(A, x) - b\n",
    "    \n",
    "    def hess_smooth(x):\n",
    "        return A\n",
    "    \n",
    "    # Define L1 proximal operator\n",
    "    proximal_l1 = lambda x, t: np.sign(x) * np.maximum(np.abs(x) - alpha * t, 0)\n",
    "    \n",
    "    # Define total objective for plotting (smooth + non-smooth)\n",
    "    def f_total(x):\n",
    "        return f_smooth(x) + alpha * np.sum(np.abs(x))\n",
    "    \n",
    "    # Subproblem solvers to test\n",
    "    solvers = ['prox-gradient', 'coordinate-descent', 'admm']\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    for i, solver in enumerate(solvers):\n",
    "        print(f\"Testing {solver} subproblem solver...\")\n",
    "        \n",
    "        # Create Proximal Newton solver\n",
    "        pn = ProximalNewton(\n",
    "            f_smooth, grad_smooth, hess_smooth, proximal_l1,\n",
    "            max_iter=30, tol=1e-6,\n",
    "            subproblem_solver=solver,\n",
    "            subproblem_iter=100,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Run optimization and get history\n",
    "        x0 = np.zeros(n)\n",
    "        iterates, num_iters = pn.optimize(x0.copy(), return_iterates=True, return_iterations=True)\n",
    "        \n",
    "        # Compute objective values and gradient norms\n",
    "        obj_values = [f_total(x) for x in iterates]\n",
    "        \n",
    "        # For non-smooth problems, compute the proximal gradient mapping norm\n",
    "        grad_mappings = []\n",
    "        for x in iterates:\n",
    "            grad_smooth_x = grad_smooth(x)\n",
    "            prox_grad_step = proximal_l1(x - grad_smooth_x, 1.0)\n",
    "            grad_mapping = np.linalg.norm(x - prox_grad_step)\n",
    "            grad_mappings.append(grad_mapping)\n",
    "        \n",
    "        # Plot results - focusing only on the gradient mapping norm for the specific plot you showed\n",
    "        plt.semilogy(grad_mappings, marker='o', label=solver)\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Proximal gradient mapping norm')\n",
    "    plt.title('Comparison of Subproblem Solvers: Optimality Measure')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('proximal_newton_subproblem_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_convergence_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e84f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
